{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJkpOMuRtXBf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTRDlmNy3O0J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "#from math import warnings\n",
        "#warnings\n",
        "#warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1MuCJEV3Q5X"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.models import Model,load_model\n",
        "conv_base =  InceptionV3(weights='imagenet',include_top=False,\n",
        "                         input_shape=(300, 300, 3))\n",
        "output = conv_base.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "model_tl = Model(conv_base.input, output)\n",
        "model_tl.trainable = False\n",
        "for layer in model_tl.layers:\n",
        "    layer.trainable = False\n",
        "layers = [(layer, layer.name, layer.trainable) for layer in  \n",
        "               model_tl.layers]\n",
        "model_layers=pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
        "print(model_layers) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skQbVQVA3xS4"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "test_size=400\n",
        "batch_size=16\n",
        "epochs=30\n",
        "train_path='/content/drive/My Drive/NEWDATA/train'\n",
        "test_path='/content/drive/My Drive/NEWDATA/val'\n",
        "target_size=(300,300) \n",
        "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3,  \n",
        "                                   rotation_range=50,\n",
        "                                   width_shift_range=0.2, \n",
        "                                   height_shift_range=0.2, \n",
        "                                   shear_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   brightness_range = [0.8, 1.2],\n",
        "                                   fill_mode='nearest',        \n",
        "                                   validation_split=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                  train_path,\n",
        "                  target_size=target_size,  \n",
        "                  batch_size=batch_size,\n",
        "                  class_mode='categorical',\n",
        "                  subset='training')\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                       train_path,\n",
        "                       target_size=target_size,\n",
        "                       batch_size=batch_size,\n",
        "                       class_mode='categorical',\n",
        "                       subset='validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8lJHZJi37wI"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "#from keras import optimizers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model =Sequential()\n",
        "model.add(model_tl)\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Checkpoint"
      ],
      "metadata": {
        "id": "tV0OqI9n5FLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S19aMshj3-UV"
      },
      "outputs": [],
      "source": [
        "\"\"\"from keras.callbacks import *\n",
        "\n",
        "filepath=\"/content/drive/My Drive/Models/epochs:{epoch:03d}-val_acc {val_acc:.3f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, \n",
        "                             monitor='val_acc', \n",
        "                             verbose=1,\n",
        "                             save_best_only=False,\n",
        "                             save_freq='epoch',     \n",
        "                             mode='max')\n",
        "callbacks_list = [checkpoint]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNKbjtOQ4tXq"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "          train_generator,\n",
        "          steps_per_epoch=train_generator.samples//batch_size,    \n",
        "          validation_data=validation_generator,\n",
        "          validation_steps= validation_generator.samples//batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          shuffle=True\n",
        "          )\n",
        "#callbacks=callbacks_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWqSNTe_4282"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "scores_train = model.evaluate(train_generator,verbose=1)\n",
        "scores_validation = model.evaluate(validation_generator,verbose=1)\n",
        "print(\"Train Accuracy: %.2f%%\" % (scores_train[1]*100))\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores_validation[1]*100))\n",
        "\n",
        "def LearningCurve(history):\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "LearningCurve(history)\n",
        "#Saving the trained model \n",
        "#model_weight_file='/content/drive/MyDrive/Models/TI_tlearn_img_aug_cnn_3.h5'\n",
        "#model.save(model_weight_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DrbhkHjCBVc"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8iMS_4UBy4U"
      },
      "outputs": [],
      "source": [
        "compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))\n",
        "test_steps = compute_steps_per_epoch(test_size)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "                 test_path,\n",
        "                 target_size=target_size, \n",
        "                 batch_size=batch_size,\n",
        "                 class_mode=None,\n",
        "                 shuffle=False)\n",
        "test_generator.reset()\n",
        "\n",
        "#make predictions\n",
        "#tl_img_aug_cnn = load_model(model_weight_file)\n",
        "pred=model.predict(test_generator,\n",
        "                            verbose=1,\n",
        "                            steps=test_steps)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "labels = (test_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices]\n",
        "filenames=test_generator.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predictions})\n",
        "\n",
        "import seaborn as sns\n",
        "def PerformanceReports(conf_matrix,class_report,labels):\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(conf_matrix, annot=True,ax=ax)\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.xaxis.set_ticklabels(labels)\n",
        "    ax.yaxis.set_ticklabels(labels)\n",
        "    plt.show()\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T,  \n",
        "                annot=True,ax=ax)\n",
        "    ax.set_title('Classification Report')\n",
        "    plt.show()\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "labels=['original','tampered']\n",
        "test_labels = [fn.split('/')[0] for fn in filenames]\n",
        "cm=confusion_matrix(test_labels,predictions)\n",
        "print(cm)\n",
        "cr=classification_report(test_labels, predictions)\n",
        "class_report=classification_report(test_labels, predictions,\n",
        "                                   target_names=labels,\n",
        "                                   output_dict=True)\n",
        "print(cr)\n",
        "PerformanceReports(cm,class_report,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13UPLTV_Dd86"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-wAP2GFCw5t"
      },
      "outputs": [],
      "source": [
        "def sample_prediction(img_path):\n",
        "    #test_im = cv2.resize(test_im, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "    #test_pred = np.argmax(model.predict_proba(test_im.reshape((1, img_size, img_size, 3))))\n",
        "    test_im = cv2.imread(img_path)\n",
        "    test_im = cv2.resize(test_im, target_size, cv2.INTER_LINEAR)/255\n",
        "    test_im = test_im.reshape((1, 300, 300, 3))\n",
        "    test_pred = model.predict(test_im)\n",
        "    test_pred = np.argmax(model(test_im))\n",
        "    return test_pred\n",
        "\n",
        "print(\"Predicted class for test_original: {}\".format(sample_prediction(\"/content/drive/MyDrive/NEWDATA/train/Original/20210316_223932.jpg\")))\n",
        "print(\"Predicted class for test_tampered: {}\".format(sample_prediction(\"/content/drive/MyDrive/NEWDATA/train/Tampered/174.jpg\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6L8AoPwDgG-"
      },
      "outputs": [],
      "source": [
        "path =os.listdir(\"/content/drive/MyDrive/TI_Test/Tampered/\")\n",
        "counto = 0\n",
        "countt = 0\n",
        "total = 0\n",
        "for file_name in path:\n",
        "    test_im = os.path.join('/content/drive/MyDrive/TI_Test/Tampered/', file_name)\n",
        "    test_im = cv2.imread(test_im)\n",
        "    test_im = cv2.resize(test_im, target_size, cv2.INTER_LINEAR) / 255\n",
        "    test_im = test_im.reshape((1, 300, 300, 3))\n",
        "    #test_pred = np.argmax(model(test_im.reshape((1, 300, 300, 3))))\n",
        "    test_pred = model.predict(test_im)\n",
        "    #print(test_pred)\n",
        "    #test_pred = np.argmax(model.predict(test_im))\n",
        "    #print(test_pred)\n",
        "    #print(\"\")\n",
        "    print(\"Filename: {} Predicted class: {}\".format(file_name,test_pred))\n",
        "    total+=1\n",
        "    if test_pred[0][1] > 0.60: #test_pred[0][1]:\n",
        "      countt+=1\n",
        "    else:\n",
        "      counto+=1\n",
        "\n",
        "print(\"Total Count: {} Count Original: {}  Count Tampered: {} \".format(total,counto,countt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBb_mnC6FldD"
      },
      "outputs": [],
      "source": [
        "path =os.listdir(\"/content/drive/MyDrive/TI_Test/Original/\")\n",
        "counto = 0\n",
        "countt = 0\n",
        "total = 0\n",
        "for file_name in path:\n",
        "    test_im = os.path.join('/content/drive/MyDrive/TI_Test/Original/', file_name)\n",
        "    test_im = cv2.imread(test_im)\n",
        "    test_im = cv2.resize(test_im, target_size, cv2.INTER_LINEAR) / 255\n",
        "    test_im = test_im.reshape((1, 300, 300, 3))\n",
        "    #test_pred = np.argmax(model(test_im.reshape((1, 300, 300, 3))))\n",
        "    test_pred = model.predict(test_im)\n",
        "    #print(test_pred)\n",
        "    #test_pred = np.argmax(model.predict(test_im))\n",
        "    \"\"\"print(test_pred)\n",
        "    print(\"\")\n",
        "    print(\"\")\"\"\"\n",
        "    print(\"Filename: {} Predicted class: {}\".format(file_name,test_pred))\n",
        "    total+=1\n",
        "    if test_pred[0][1] > 0.60: # test_pred[0][1]:\n",
        "      countt+=1\n",
        "    else:\n",
        "      counto+=1\n",
        "\n",
        "print(\"Total Count: {} Count Original: {}  Count Tampered: {} \".format(total,counto,countt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It0aJ1NRkvDH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}